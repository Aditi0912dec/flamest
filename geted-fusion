import pickle
import torch
import torch
import torch.nn as nn
import torch.nn.functional as F
text_embeddings= torch.load("/scratch/cs21d001/action_recognition/vlm/align_final_embeddings_train_ucf101.pt")  # List of tensors, each of shape (1, 768)
with open('/scratch/cs21d001/action_recognition/vlm/ucf101_slow_train_img.pkl', 'rb') as f:
    cnn_embeddings=  pickle.load(f)
cnn_tensor = torch.stack(cnn_embeddings).squeeze(1)   # Shape: [100, 1024]
text_tensor = torch.stack(text_embeddings).squeeze(1) # Shape: [100, 768]


print("CNN tensor shape:", cnn_tensor.shape)    # torch.Size([100, 1024])
print("Text tensor shape:", text_tensor.shape)  # torch.Size([100, 768])



class ResidualGatedCrossFusion(nn.Module):
    def __init__(self, img_dim, txt_dim, embed_dim):
        super().__init__()
        self.img_proj = nn.Linear(img_dim, embed_dim)
        self.txt_proj = nn.Linear(txt_dim, embed_dim)

        self.W_q_v = nn.Linear(embed_dim, embed_dim)
        self.W_k_t = nn.Linear(embed_dim, embed_dim)
        self.W_v_t = nn.Linear(embed_dim, embed_dim)

        self.W_q_t = nn.Linear(embed_dim, embed_dim)
        self.W_k_v = nn.Linear(embed_dim, embed_dim)
        self.W_v_v = nn.Linear(embed_dim, embed_dim)

        self.W_g_v = nn.Linear(2 * embed_dim, embed_dim)
        self.W_g_t = nn.Linear(2 * embed_dim, embed_dim)

        self.fusion_mlp = nn.Sequential(
            nn.Linear(2 * embed_dim, 2 * embed_dim),
            nn.ReLU(),
            nn.Linear(2 * embed_dim, 2 * embed_dim)
        )

    def forward(self, v_raw, t_raw):
        v = self.img_proj(v_raw)
        t = self.txt_proj(t_raw)

        q_v = self.W_q_v(v)
        k_t = self.W_k_t(t)
        v_t = self.W_v_t(t)
        attn_weights_v = torch.softmax(torch.matmul(q_v, k_t.T) / (v.shape[-1] ** 0.5), dim=-1)
        a_v = torch.matmul(attn_weights_v, v_t)

        q_t = self.W_q_t(t)
        k_v = self.W_k_v(v)
        v_v = self.W_v_v(v)
        attn_weights_t = torch.softmax(torch.matmul(q_t, k_v.T) / (t.shape[-1] ** 0.5), dim=-1)
        a_t = torch.matmul(attn_weights_t, v_v)

        concat_v = torch.cat([v, a_v], dim=-1)
        concat_t = torch.cat([t, a_t], dim=-1)
        g_v = torch.sigmoid(self.W_g_v(concat_v))
        g_t = torch.sigmoid(self.W_g_t(concat_t))

        v_hat = g_v * a_v + (1 - g_v) * v
        t_hat = g_t * a_t + (1 - g_t) * t

        fused_input = torch.cat([v_hat, t_hat], dim=-1)
        fused_output = self.fusion_mlp(fused_input) + fused_input

        return fused_output


embed_dim = 256  # You can change this

fusion_model = ResidualGatedCrossFusion(img_dim=2048, txt_dim=768, embed_dim=embed_dim)
fused_embeddings = fusion_model(cnn_tensor, text_tensor)

print("Fused embeddings shape:", fused_embeddings.shape)  # torch.Size([100, 512])

with open('/scratch/cs21d001/action_recognition/vlm/gated_fused_train_ucf101.pkl', 'wb') as f:
     pickle.dump(fused_embeddings, f)

print("DONE with training embeddings")

################


torch.save(fusion_model.state_dict(), '/scratch/cs21d001/action_recognition/vlm/fusion_classifier_ucf101.pth')

# Load
model = ResidualGatedCrossFusion(img_dim=2048, txt_dim=768, embed_dim=embed_dim)

# Load weights
model.load_state_dict(torch.load("/scratch/cs21d001/action_recognition/vlm/fusion_classifier_ucf101.pth"))
model.eval()

text_test= torch.load("/scratch/cs21d001/action_recognition/vlm/align_final_embeddings_test_ucf101.pt")  # List of tensors, each of shape (1, 768)
#final_embeddings_test= torch.load("/scratch/cs21d001/action_recognition/vlm/align_final_embeddings_test_hmdb51.pt") 

with open('/scratch/cs21d001/action_recognition/vlm/ucf101_slow_test_img.pkl', 'rb') as f:
    cnn_test=  pickle.load(f)



# Stack and squeeze
cnn_test_tensor = torch.stack(cnn_test).squeeze(1)   # Shape: [N, 2048]
text_test_tensor = torch.stack(text_test).squeeze(1) # Shape: [N, 768]

with torch.no_grad():
    fused_features = model(cnn_test_tensor, text_test_tensor)  # Shape: [N, 256]

with open('/scratch/cs21d001/action_recognition/vlm/gated_fused_test_ucf101.pkl', 'wb') as f:
     pickle.dump(fused_features, f)
print("Fused embeddings shape:", fused_features.shape)  # torch.Size([100, 512])
