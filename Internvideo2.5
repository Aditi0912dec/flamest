import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pickle, random
from torch.utils.data import DataLoader
from collections import defaultdict
from sklearn.model_selection import train_test_split

# ==========================================================
# üß† Device Setup
# ==========================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ Using device: {device}")

# ==========================================================
# üìÇ Load Data
# ==========================================================
base_path = "/data/2024_July_07_Aditi"

gt_train = np.load(f"{base_path}/ucf101_labels_train.npy")
gt_test = np.load(f"{base_path}/ucf101_labels_test.npy")

with open(f"{base_path}/ucf101_iv2_train_emb.pkl", 'rb') as f:
    combined_embeddings_train = pickle.load(f)
with open(f"{base_path}/ucf101_iv2_test_emb.pkl", 'rb') as f:
    combined_embeddings_test = pickle.load(f)

data, labels = combined_embeddings_train, gt_train
test_data, test_labels = combined_embeddings_test, gt_test

num_classes = len(np.unique(gt_train))
print(f"‚úÖ Number of classes: {num_classes}")
print(f"‚úÖ Sample embedding shape: {data[0].shape}")

# ==========================================================
# ‚öôÔ∏è Helper Functions
# ==========================================================
def create_label_to_data(labels):
    label_to_data = {label: [] for label in set(labels)}
    for idx, label in enumerate(labels):
        label_to_data[label].append(idx)
    return label_to_data


# def dirichlet_client_wise(n, k, m, label_to_data, samples_per_client=1000, seed=42):
#     np.random.seed(seed)
#     random.seed(seed)
#     client_data_indices = defaultdict(list)

#     for client_id in range(n):
#         proportions = np.random.dirichlet([m] * k)  # Dirichlet proportions per label
#         num_samples = samples_per_client
#         for label, proportion in enumerate(proportions):
#             label_data = label_to_data[label]
#             count = int(round(proportion * num_samples))
#             if count > 0:
#                 sampled = random.sample(label_data, min(count, len(label_data)))
#                 client_data_indices[client_id].extend(sampled)
#     return client_data_indices

def dirichlet_label_distribution(n, k, m, label_to_data):
    """
    Distribute k labels across n clients using Dirichlet distribution.
    
    Parameters:
    - n: Number of clients
    - k: Number of labels
    - m: Concentration parameter (Œ≤) controlling skewness
    - label_to_data: Dictionary mapping labels to their data points
    
    Returns:
    - client_data_indices: Dictionary mapping client_id to data indices
    """
    # Step 1: Draw proportions for each label across clients
    label_proportions = np.random.dirichlet([m] * n, k)

    # Step 2: Assign data points to clients
    client_data_indices = defaultdict(list)

    for label, proportions in enumerate(label_proportions):
        # Shuffle data points for this label
        data_points = label_to_data[label]
        random.shuffle(data_points)

        # Allocate data points to clients
        start = 0
        for client_id, proportion in enumerate(proportions):
            num_samples = int(proportion * len(data_points))
            client_data_indices[client_id].extend(data_points[start:start + num_samples])
            start += num_samples

    return client_data_indices


# ==========================================================
# üîÄ Dirichlet Split
# ==========================================================
n = 1
k = num_classes
m = 0.6
label_to_data = create_label_to_data(labels)
client_data_indices = dirichlet_label_distribution(n, k, m, label_to_data)



# ==========================================================
# üß© Client Class
# ==========================================================
class Client:
    def __init__(self, client_id, data, labels, batch, num_classes):
        self.client_id = client_id
        self.batch_size = batch
        self.train_data = data
        self.train_labels = labels

        first = torch.as_tensor(self.train_data[0], dtype=torch.float32)
        feat_dim = first.shape[-1] if first.dim() > 1 else first.shape[0]

        self.mlp = self.build_mlp(input_dim=feat_dim, hidden_dims=[512, 256], output_dim=num_classes).to(device)

    def build_mlp(self, input_dim, hidden_dims, output_dim):
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        return nn.Sequential(*layers)

    def train(self, epochs=80, lr=0.001):
        optimizer = optim.Adam(self.mlp.parameters(), lr=lr)
        loss_fn = nn.CrossEntropyLoss()

        def collate(batch):
            frames, labels = zip(*batch)
            return list(frames), torch.tensor(labels)

        train_loader = DataLoader(list(zip(self.train_data, self.train_labels)),
                                  batch_size=self.batch_size, shuffle=True, collate_fn=collate)

        self.mlp.train()
        for epoch in range(epochs):
            epoch_loss = 0.0
            for batch in train_loader:
                frames_batch, labels_batch = batch
                frames_tensors = [torch.as_tensor(f, dtype=torch.float32, device=device) for f in frames_batch]
                frames_batch_tensor = torch.stack(frames_tensors, dim=0)
                labels_tensor = torch.as_tensor(labels_batch, dtype=torch.long, device=device)

                output = self.mlp(frames_batch_tensor)
                loss = loss_fn(output, labels_tensor)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
        return self.mlp

    def test(self):
        def collate(batch):
            frames, labels = zip(*batch)
            return list(frames), torch.tensor(labels)

        test_loader = DataLoader(list(zip(self.train_data, self.train_labels)),
                                 batch_size=self.batch_size, shuffle=False, collate_fn=collate)

        self.mlp.eval()
        correct, total = 0, 0
        loss_fn = nn.CrossEntropyLoss()
        total_loss = 0.0

        with torch.no_grad():
            for batch in test_loader:
                frames_batch, labels_batch = batch
                frames_tensors = [torch.as_tensor(f, dtype=torch.float32, device=device) for f in frames_batch]
                frames_batch_tensor = torch.stack(frames_tensors, dim=0)
                labels_tensor = torch.as_tensor(labels_batch, dtype=torch.long, device=device)

                output = self.mlp(frames_batch_tensor)
                loss = loss_fn(output, labels_tensor)
                total_loss += loss.item()

                pred = torch.argmax(output, dim=1)
                correct += (pred == labels_tensor).sum().item()
                total += len(labels_tensor)

        accuracy = (correct / total) * 100 if total > 0 else 0.0
        return accuracy


# ==========================================================
# üåê Server Class
# ==========================================================
class Server:
    def __init__(self, input_dim, hidden_dims=[512, 256], output_dim=num_classes):
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        self.global_mlp = nn.Sequential(*layers).to(device)

    def aggregate_models(self, client_models):
        mlp_state_dicts = [client.state_dict() for client in client_models]

        with torch.no_grad():
            global_mlp_state_dict = self.global_mlp.state_dict()
            for key in global_mlp_state_dict:
                if "num_batches_tracked" in key:
                    continue
                client_weights = [sd[key] for sd in mlp_state_dicts]
                global_mlp_state_dict[key] = torch.stack(client_weights).mean(dim=0)
            self.global_mlp.load_state_dict(global_mlp_state_dict)
        return self.global_mlp

    def distribute_model(self, clients):
        for client in clients:
            client.mlp.load_state_dict(self.global_mlp.state_dict())

    def server_test(self, global_model, test_data, test_labels, batch_size):
        def collate(batch):
            frames, labels = zip(*batch)
            return list(frames), torch.tensor(labels)

        test_loader = DataLoader(list(zip(test_data, test_labels)),
                                 batch_size=batch_size, shuffle=False, collate_fn=collate)

        global_model.eval()
        correct, total = 0, 0
        loss_fn = nn.CrossEntropyLoss()
        total_loss = 0.0

        with torch.no_grad():
            for batch in test_loader:
                frames_batch, labels_batch = batch
                frames_tensors = [torch.as_tensor(f, dtype=torch.float32, device=device) for f in frames_batch]
                frames_batch_tensor = torch.stack(frames_tensors, dim=0)
                labels_tensor = torch.as_tensor(labels_batch, dtype=torch.long, device=device)

                output = global_model(frames_batch_tensor)
                loss = loss_fn(output, labels_tensor)
                total_loss += loss.item()

                pred = torch.argmax(output, dim=1)
                correct += (pred == labels_tensor).sum().item()
                total += len(labels_tensor)

        accuracy = (correct / total) * 100 if total > 0 else 0.0
        return accuracy


# ==========================================================
# üöÄ Federated Learning Execution
# ==========================================================
rounds = 1
batch = 128
clients = []
for i in range(n):
        indices = client_data_indices[i]
        client_data = [data[j] for j in indices]
        client_labels = [labels[j] for j in indices]
        clients.append(Client(i, client_data, client_labels, batch, num_classes))

feat_dim = torch.as_tensor(data[0], dtype=torch.float32).shape[-1]
print(f"‚úÖ Feature dimension for MLP: {feat_dim}")


    

mean_acc = []
server = Server(input_dim=feat_dim)


for r in range(rounds):
        local_models = [client.train() for client in clients]
        global_model = server.aggregate_models(local_models)
        acc = server.server_test(global_model, test_data, test_labels, batch)
        mean_acc.append(acc)
        server.distribute_model(clients)

print(f"‚úÖ Avg Accuracy after {rounds} rounds: {np.mean(mean_acc):.2f}%")
print(f"‚úÖ Max Accuracy: {max(mean_acc):.2f}% at round {np.argmax(mean_acc)+1}")
print(f"‚úÖ Min Accuracy: {min(mean_acc):.2f}% at round {np.argmin(mean_acc)+1}")

print("\nüéâ DONE!")


############

import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer
import os, gc

# ----------------------------------------------------------------------
# 1Ô∏è‚É£ Memory cleanup and config
# ----------------------------------------------------------------------
gc.collect()
torch.cuda.empty_cache()
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

device = "cuda" if torch.cuda.is_available() else "cpu"

# ----------------------------------------------------------------------
# 2Ô∏è‚É£ Load model and tokenizer
# ----------------------------------------------------------------------
model_path = "OpenGVLab/InternVideo2_5_Chat_8B"
DATASET_DIR_TRAIN = "/data/2024_July_07_Aditi/hmdb51_train_dataset"
CLASSES = os.listdir(DATASET_DIR_TRAIN)

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

try:
    model = AutoModel.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    ).eval()
except RuntimeError:
    print("‚ö†Ô∏è GPU OOM ‚Äî switching to CPU...")
    model = AutoModel.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float32
    ).to("cpu").eval()

print(f"‚úÖ Model loaded on {next(model.parameters()).device}")

# ----------------------------------------------------------------------
# 3Ô∏è‚É£ Frame preprocessing utils
# ----------------------------------------------------------------------
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size=448):
    return T.Compose([
        T.Lambda(lambda img: img.convert("RGB") if img.mode != "RGB" else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])

def load_frames(frame_paths, input_size=448):
    transform = build_transform(input_size)
    pixel_values = []
    for frame_path in frame_paths:
        img = Image.open(frame_path).convert("RGB")
        img_tensor = transform(img)
        pixel_values.append(img_tensor)
    pixel_values = torch.stack(pixel_values)
    num_patches_list = [1] * len(frame_paths)
    return pixel_values, num_patches_list

# ----------------------------------------------------------------------
# 4Ô∏è‚É£ Extract embeddings
# ----------------------------------------------------------------------
def get_frame_embeddings_internvideo(model, pixel_values, num_patches_list):
    embeddings_list = []
    start = 0
    model_dtype = next(model.parameters()).dtype
    
    for n_patches in num_patches_list:
        batch = pixel_values[start:start+n_patches].to(device)
        batch = batch.to(model_dtype)
        with torch.no_grad():
            outputs = model.vision_model(batch)
            if hasattr(outputs, "last_hidden_state"):
                outputs = outputs.last_hidden_state[:, 0, :]  # CLS token
        embeddings_list.append(outputs.cpu())
        start += n_patches

    return torch.cat(embeddings_list)

# ----------------------------------------------------------------------
# 5Ô∏è‚É£ Main pipeline
# ----------------------------------------------------------------------
# Example: paths to 3 frames of one video
video_vector=[]
def get_frames(action_classes,dataset_dir):
    count=0
    video_paths,labels=[],[]
   # c_index=-1
    for class_index,action_class in enumerate(action_classes):
       
        action_class_path = os.path.join(dataset_dir , action_class)
        counter = 0
        if os.path.isdir(action_class_path):  # Check if it's a directory
            # Get video files from the action class folder
            video_files = [os.path.join(action_class_path, f) for f in os.listdir(action_class_path) if f.endswith('.avi')]
        # train_videos, test_videos = train_test_split(video_files, test_size=0.2, random_state=42)


            for video_file in video_files:
                frame_paths = sorted(os.listdir(video_file))
                pixel_values, num_patches_list = load_frames(frame_paths)
                video_embeddings = get_frame_embeddings_internvideo(model, pixel_values, num_patches_list)
                video_vector.append(video_embeddings.mean(dim=0))
                labels.append(class_index)
                break
                    
                         #   counter=counter+1
            print(f"action class is {action_class} and the sample count is {len(video_files)}")
            # count=count+1
            # #print(count)
            # if count == 3:
        break
    return video_vector,labels



video_emb,gt_train=get_frames(CLASSES,DATASET_DIR_TRAIN,8)
np.save("/data/2024_July_07_Aditi/hmdb51_labels_train.npy",gt_train)

# np.save("/scratch/cs21d001/action_recognition/vlm/labels_test.npy",gt_test)

with open('/data/2024_July_07_Aditi/hmdb51_iv2_train_emb.pkl', 'wb') as f:
   pickle.dump(video_emb,f)


print("üíæ Embeddings saved to disk")

#############
