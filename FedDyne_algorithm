from sklearn.model_selection import train_test_split
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from collections import defaultdict
import random
import pickle
from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score

# Assuming data is already loaded as in your code
final_embeddings_train = torch.load("/scratch/cs21d001/action_recognition/vlm/align_final_embeddings_train_hmdb51.pt")
final_embeddings_test = torch.load("/scratch/cs21d001/action_recognition/vlm/align_final_embeddings_test_hmdb51.pt")
gt_train = np.load("/scratch/cs21d001/action_recognition/vlm/hmdb51_labels_train.npy")
gt_test = np.load("/scratch/cs21d001/action_recognition/vlm/hmdb51_labels_test.npy")

with open('/scratch/cs21d001/action_recognition/vlm/hmdb51_slow_train_img.pkl', 'rb') as f:
    img_emb_train = pickle.load(f)
with open('/scratch/cs21d001/action_recognition/vlm/hmdb51_slow_test_img.pkl', 'rb') as f:
    img_emb_test = pickle.load(f)

# Combine image and text embeddings
combined_embeddings_train = [torch.cat((img, text), dim=1) for img, text in zip(img_emb_train, final_embeddings_train)]
combined_embeddings_test = [torch.cat((img, text), dim=1) for img, text in zip(img_emb_test, final_embeddings_test)]

input_dim = combined_embeddings_train[0].shape[1]
num_classes = len(np.unique(gt_train))

print(f"Number of classes: {num_classes}")
for i in range(num_classes):
    print(f"For class {i}, number of samples: {len(np.where(gt_train == i)[0])}")

# Dirichlet distribution for non-IID data
def create_label_to_data(labels):
    """Creates a mapping from label to its corresponding data indices."""
    label_to_data = {label: [] for label in set(labels)}
    for idx, label in enumerate(labels):
        label_to_data[label].append(idx)
    return label_to_data

def dirichlet_label_distribution(n, k, m, label_to_data):
    """
    Distribute k labels across n clients using Dirichlet distribution.
    
    Parameters:
    - n: Number of clients
    - k: Number of labels
    - m: Concentration parameter (Î²) controlling skewness
    - label_to_data: Dictionary mapping labels to their data points
    
    Returns:
    - client_data_indices: Dictionary mapping client_id to data indices
    """
    label_proportions = np.random.dirichlet([m] * n, k)
    client_data_indices = defaultdict(list)
    
    for label, proportions in enumerate(label_proportions):
        data_points = label_to_data[label]
        random.shuffle(data_points)
        start = 0
        for client_id, proportion in enumerate(proportions):
            num_samples = int(proportion * len(data_points))
            client_data_indices[client_id].extend(data_points[start:start + num_samples])
            start += num_samples
    
    return client_data_indices

data, labels = combined_embeddings_train, gt_train
test_data, test_labels = combined_embeddings_test, gt_test
num_classes = len(np.unique(labels))
print(f"Number of classes: {num_classes}")

n = 4  # Number of clients
k = num_classes  # Total labels
m = 1# Dirichlet concentration parameter
label_to_data = create_label_to_data(labels)
client_data_indices = dirichlet_label_distribution(n, k, m, label_to_data)

# Client class with FedDyn regularization
class Client:
    def __init__(self, client_id, data, labels, batch, num_classes, alpha=0.01):
        self.client_id = client_id
        self.data = data
        self.labels = labels
        self.batch_size = batch
        self.train_data = data
        self.train_labels = labels
        self.alpha = alpha  # FedDyn regularization strength
        self.mlp = self.build_mlp(input_dim=input_dim, hidden_dims=[512, 256], output_dim=num_classes)
        # Initialize local gradient (h_k in FedDyn)
        self.local_grad = [torch.zeros_like(param) for param in self.mlp.parameters()]

    def build_mlp(self, input_dim, hidden_dims, output_dim):
        """Builds and returns the MLP model."""
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        model = nn.Sequential(*layers)
        return model

    def train(self, epochs=10, lr=0.01, server_model_state_dict=None):
        optimizer = optim.Adam(self.mlp.parameters(), lr=lr)
        loss_fn = nn.CrossEntropyLoss()

        def custom_collate(batch):
            frames, labels = zip(*batch)
            return list(frames), torch.tensor(labels)

        train_loader = DataLoader(
            list(zip(self.train_data, self.train_labels)), 
            batch_size=self.batch_size, 
            shuffle=True, 
            collate_fn=custom_collate
        )

        # Load server model parameters for regularization
        if server_model_state_dict:
            server_model = self.build_mlp(input_dim=input_dim, hidden_dims=[512, 256], output_dim=num_classes)
            server_model.load_state_dict(server_model_state_dict)
            global_params = list(server_model.parameters())
        else:
            global_params = [param.clone().detach() for param in self.mlp.parameters()]

        for epoch in range(epochs):
            epoch_loss = 0.0
            for batch in train_loader:
                frames_batch, labels_batch = batch
                frames_batch = torch.cat([torch.tensor(frame, dtype=torch.float32) for frame in frames_batch])
                label_tensor = torch.tensor(labels_batch, dtype=torch.long)

                optimizer.zero_grad()
                output = self.mlp(frames_batch)
                loss_mlp = loss_fn(output, label_tensor)

                # FedDyn regularization: alpha * sum((theta - theta_global) * h_k)
                reg_loss = 0
                for lp, gp, lg in zip(self.mlp.parameters(), global_params, self.local_grad):
                    reg_loss += self.alpha * torch.sum((lp - gp) * lg)
                total_loss = loss_mlp + reg_loss

                total_loss.backward()
                optimizer.step()
                epoch_loss += total_loss.item()

            #print(f"Client {self.client_id}, Epoch {epoch+1}: Training Loss = {epoch_loss / len(train_loader):.4f}")

        # Update local gradient (h_k in FedDyn)
        with torch.no_grad():
            for lg, lp, gp in zip(self.local_grad, self.mlp.parameters(), global_params):
                lg.copy_(lg - self.alpha * (lp - gp))

        return self.mlp

# Federated Server Class
class Server:
    def __init__(self, input_dim=input_dim, hidden_dims=[512, 256], output_dim=num_classes):
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        self.global_mlp = nn.Sequential(*layers)

    def aggregate_models(self, client_models):
        """Aggregate weights of MLP using FedDyn (similar to FedAvg)."""
        mlp_state_dicts = [client.state_dict() for client in client_models]
        with torch.no_grad():
            global_mlp_state_dict = self.global_mlp.state_dict()
            for key in global_mlp_state_dict:
                if "num_batches_tracked" in key:
                    continue
                client_weights = [client_state_dict[key] for client_state_dict in mlp_state_dicts]
                if not all(w.shape == client_weights[0].shape for w in client_weights):
                    raise ValueError(f"Shape mismatch in MLP parameter '{key}' across clients.")
                global_mlp_state_dict[key] = torch.stack(client_weights).mean(dim=0)
            self.global_mlp.load_state_dict(global_mlp_state_dict)
        return self.global_mlp

    def server_test(self, global_model, test_data, test_labels, batch_size):
        self.batch_size = batch_size
        def custom_collate(batch):
            frames, labels = zip(*batch)
            return list(frames), torch.tensor(labels)
    
        test_loader = DataLoader(
            list(zip(test_data, test_labels)), 
            batch_size=self.batch_size, 
            shuffle=False, 
            collate_fn=custom_collate
        )

        correct, total = 0, 0
        loss_fn = nn.CrossEntropyLoss()
        total_loss = 0.0
        y_true, y_pred = [], []
        with torch.no_grad():
            for batch in test_loader:
                frames_batch, labels_batch = batch
                frames_batch = torch.cat([torch.tensor(frame, dtype=torch.float32) for frame in frames_batch])
                label_tensors = torch.tensor(labels_batch, dtype=torch.long)
                
                output = global_model(frames_batch)
                loss = loss_fn(output, label_tensors)
                total_loss += loss.item()

                pred = torch.argmax(output, dim=1)
                y_pred.extend(pred.cpu().numpy())
                y_true.extend(label_tensors.cpu().numpy())
                correct += (pred == label_tensors).sum().item()
                total += len(label_tensors)
        
        accuracy = (correct / total) * 100
        p.append(precision_score(y_true, y_pred, average='macro'))
        r.append(recall_score(y_true, y_pred, average='macro'))
        f1.append(f1_score(y_true, y_pred, average='macro'))
        b.append(balanced_accuracy_score(y_true, y_pred))
        return accuracy

    def distribute_model(self, clients):
        """Distribute updated global model to all clients."""
        if not clients:
            print("Warning: No clients found to distribute models to.")
            return
        for client in clients:
            client.mlp.load_state_dict(self.global_mlp.state_dict())

# Initialize clients
clients = []
batch = 128
p, r, f1, b = [], [], [], []

for i in range(n):
    data_indices = client_data_indices[i]
    client_data = [data[j] for j in data_indices]
    client_labels_data = [labels[j] for j in data_indices]
    clients.append(Client(i, client_data, client_labels_data, batch, num_classes, alpha=0.01))
    #print(f"Client {i} has labels: {np.unique(client_labels_data)}")

# Federated Learning Loop
server = Server()
mean_acc = []
rounds = 1

for round in range(rounds):
    #print(f"Currently on round: {round+1}")
    local_models = [client.train(server_model_state_dict=server.global_mlp.state_dict()) for client in clients]
    global_model = server.aggregate_models(local_models)
    mean_acc.append(server.server_test(global_model, test_data, test_labels, batch))
    #print(f"For round {round+1} the accuracy on test dataset is {mean_acc[round]:.2f}")
    server.distribute_model(clients)

# Print results
print(f"Average accuracy achieved after round {round+1}: {np.mean(mean_acc):.2f}%")
print(f"Maximum accuracy achieved at round {mean_acc.index(max(mean_acc))+1}: {max(mean_acc):.2f}%")
print(f"Minimum accuracy achieved at round {mean_acc.index(min(mean_acc))+1}: {min(mean_acc):.2f}%")
print(f"Average precision achieved after round {round+1}: {np.mean(p)*100:.2f}%")
print(f"Average recall achieved after round {round+1}: {np.mean(r)*100:.2f}%")
print(f"Average F1 achieved after round {round+1}: {np.mean(f1)*100:.2f}%")
print(f"Average balanced accuracy achieved after round {round+1}: {np.mean(b)*100:.2f}%")

print(f"Length of combined_embeddings train: {len(combined_embeddings_train)}")
print(f"Shape of combined_embeddings[0] train: {combined_embeddings_train[0].shape}")
print(f"Length of combined_embeddings test: {len(combined_embeddings_test)}")
print(f"Shape of combined_embeddings[0] test: {combined_embeddings_test[0].shape}")

print("DONE")
